---
title       : Neurodata Lunch Talk -- Circle Experiment 
author      : Jesse L. Patsolic
date        : 20180711
output      : 
  beamer_presentation:
    keep_tex: true
    theme: "Berlin"
    colortheme: "dolphin"
    fonttheme:  "structurebold"
---

```{r, echo=FALSE, eval=FALSE}
require(rmarkdown)
knitr::opts_chunk$set(warning = FALSE, message = FALSE, cache = FALSE, dpi = 300)

rmarkdown::render("index.Rmd")
#system('open index.html')
#system('mupdf-gl index.pdf &')
#system('open index.pdf')
```


```{r packages, include = FALSE, echo = FALSE}
require(ggplot2)
require(foreach)
require(grDevices)
require(rmarkdown)
require(rerf)
require(devtools)
require(doMC)
registerDoMC(8)
```

# Motivation
The recent arXiv paper [*Diffeomorphic Learning*](https://arxiv.org/abs/1806.01240) by L. Younes in the
CIS/AMS department brought up an interesting problem.  

Deep learning and diffeomorphic learning have similar properties. 
The only algorithm that did well on the following problem was **MLP**.

## Problem statement 

Consider a 100-gon where 10 vertices are colored 'black' and
the rest are colored 'white' according to the following rules: 

* Class 1: two groups of 5 contiguous vertices colored 1.
* Class 2: two groups of 6 and 4 contiguous vertices colored 1.


```{r, echo = FALSE, include = FALSE}
list2env(readRDS('scripts/toy_circle_data_split50.Rdat'), .GlobalEnv)

run1 <- readRDS("scripts/R-RerF_image-patch.Rdat")
run2 <- readRDS("scripts/R-RerF_ts-patch.Rdat")

n <- 1/2 * c(400, 800, 2000, 4000)
c1 <- c2 <- n
```


# Visualizing some of the data as an image

```{r viz1, fig.width = 10, fig.height = 10, echo = FALSE}
set.seed(1030)
v <- as.raster(Xtrain[[1]][sort(sample(nrow(Xtrain[[1]]),100)),])
plot(v)
```

# Visualizing some of the data as circles

```{r viz2, fig.width = 8, fig.height = 4, echo = FALSE}
cs <- seq(0, 2 * pi - (2 * pi / 100), length = 100)

x <- lapply(cs, sin)
y <- lapply(cs, cos)

set.seed(317)
ji <- c(sample(which(trainDat[[1]]$Y == 0), 3), sample(which(trainDat[[1]]$Y == 1), 3))

par(mfrow = c(2,3))
for(j in ji) {
  par(mar = c(1,1,1,1))
  ccols <- as.numeric(trainDat[[1]][j, -1] + 1)
  plot(x, y, col = c('gray90', 'red')[ccols], asp = 1,
       pch = 20, xaxt='n', yaxt='n', xlab = '', ylab = '',
       cex = 1)
  text(0,0, label = ifelse(trainDat[[1]][j, 1] == 0, 'Class 1', 'Class 2'))
}
```

# Structured RerF (S-RerF)

Structured RerF is a variant of RerF that takes into account the spatial
dependency of pixels in images.  

The data are taken down decision trees and at each split node S-RerF
randomly samples $d$ patches of contiguous pixels. The constructed
features are then linear combinations with random weights of the
patches.  



# image-patch-rectangles

Below are the results of each run sweeping over $w \in [2,15]$ for each of the $n$.
Horizontal lines are plotted at 0.05 and 0.01.

```{r ggplot, echo = FALSE}
p1 <- ggplot(data = run1, aes(x = patch.max)) + 
	scale_y_log10() + 
	geom_point(aes(y = oob.error, col = "oob.error", shape = "oob.error"), alpha = 0.75) + 
	geom_point(aes(y = training.error, col = "train", shape = "train"), alpha = 0.75) + 
	geom_point(aes(y = testing.error, col = "test", shape = "test"), alpha = 0.75) + 
	geom_hline(yintercept = c(0.01, 0.05), size = 0.1, colour = 'gray10') + 
	#geom_point(alpha = 0.65, aes(shape = as.factor(n))) + 
	facet_grid(n ~ d, labeller = labeller(n = label_both, d = label_both))
```

# Plot of parameter sweep

```{r, fig.width = 10, fig.height = 6, echo = FALSE}
print(p1)
```

# Best $\hat{L}$

```{r tab1, include = FALSE, echo = FALSE}
mtab <- sapply(n, function(nt) {
       tmp <- which.min(run1[run1$n == nt, ]$testing.error)
       identity(run1[run1$n == nt, ][tmp,])
		   })
```

```{r, results = 'asis', echo = FALSE}
knitr::kable(t(mtab)[, c(1,2,3,6,4,5)])
```


```{r, ggplot_run2, echo = FALSE}
p3.testing <- 
  ggplot(data = run2,
             aes(x = patch.min, y = patch.max)) + 
       geom_raster(aes(fill = testing.error)) + 
       scale_fill_gradient2(
         low  = "#006600", 
         mid  = "white", 
         high = "#000000", 
         midpoint = log10(0.05), 
         trans = 'log10',
         breaks = c(0.001, 0.05, 0.01,0.1,1)) + 
       facet_grid(n ~ d, labeller = labeller(d = label_both, n = label_both))

p3.training <- 
  ggplot(data = run2,
             aes(x = patch.min, y = patch.max)) + 
       geom_raster(aes(fill = training.error)) + 
       scale_fill_gradient2(
         low  = "#006600", 
         mid  = "white", 
         high = "#000000", 
         midpoint = log10(0.05), 
         trans = 'log10',
         breaks = c(0.001, 0.05, 0.01,0.1,1)) + 
       facet_grid(n ~ d, labeller = labeller(d = label_both, n = label_both))

p3.oob <- 
  ggplot(data = run2,
             aes(x = patch.min, y = patch.max)) + 
       geom_raster(aes(fill = oob.error)) + 
       scale_fill_gradient2(
         low  = "#006600", 
         mid  = "white", 
         high = "#000000", 
         midpoint = log10(0.05), 
         trans = 'log10',
         breaks = c(0.001, 0.05, 0.01,0.1,1)) + 
       facet_grid(n ~ d, labeller = labeller(d = label_both, n = label_both))

#show(gridExtra::grid.arrange(p3.training, p3.oob, p3.testing))
#p3.testing     
```
 
# Sweep over patch.min and patch.max

```{r ggplot3, fig.width = 14, fig.height = 8, echo = FALSE, dpi = 96, echo = FALSE}
plot(p3.testing)
pdf("p3.pdf", height = 8, width = 14)
plot(p3.testing)
dev.off()
```


# Best $\hat{L}$

```{r tab2, include = FALSE, echo = FALSE}
mtab2 <- sapply(n, function(nt) {
       tmp <- which.min(run2[run2$n == nt, ]$testing.error)
       identity(run2[run2$n == nt, ][tmp,])
		   })
```

```{r, results = 'asis', echo = FALSE, fig.width = 5}
knitr::kable(t(mtab2)[, c(1,2,7,3,4)])
```

# Merge of table 5.

\tiny

| training samples |Log. Reg. | Lin. SVM | SVM   | RF  | kNN | MLP | R-RerF |
|:----------------:|:---------|:---------|:------|:----|:----|:----|:-------|
|   200            | 0.513    | 0.515    | 0.460 |0.505|0.532|0.411|0.09    |
|   400            | 0.465    | 0.467    | 0.498 |0.497|0.488|0.144|0.045   |
|   1000           | 0.543    | 0.549    | 0.450 |0.499|0.403|0.024|0.037   |
|   2000           | 0.514    | 0.512    | 0.442 |0.510|0.283|0.013|0.03    |



```{r, echo = FALSE, include = FALSE}
comp <- data.frame( 
          training_samples = c(200, 400, 1000, 2000),
          Log.Reg = c(0.513,0.465,0.543, 0.514),
          Lin.SVM =  c(0.515, 0.467, 0.549, 0.512),
          SVM  =     c(0.460, 0.498, 0.450, 0.442),
          RF =       c(0.505, 0.497, 0.499, 0.510),
          kNN =      c(0.532, 0.488, 0.403, 0.283),
          MLP =      c(0.411, 0.144, 0.024, 0.013),
          R.RerF  =  c(0.09 , 0.045, 0.037, 0.03)
          )

dat <- cbind(reshape2::melt(comp[, -1]), training.samples = comp$train)
dat$algorithm <- as.factor(dat$variable)
dat$fraction.error <- dat$value
head(dat)

p.comp <- 
  ggplot(data = dat, aes(x = training.samples, y = fraction.error, colour = algorithm)) + 
  geom_point() + 
  scale_y_continuous(breaks = c(0.01, seq(0.0,0.7, by = 0.05))) + 
  #geom_hline(yintercept = c(0.05, 0.01)) + 
  geom_line(alpha = 0.75)
```

# Comparison of R-Rerf to other algorithms

```{r, echo = FALSE, fig.width = 12}
plot(p.comp)
```






